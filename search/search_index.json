{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dammit is a simple de novo transcriptome annotator. It was born out of the observation that annotation is mundane and annoying, all the individual pieces of the process exist already, and the existing solutions are overly complicated or rely on crappy non-free software. Your PI, wistfully thinking back on Perl4. Science shouldn't suck for the sake of sucking, so dammit attempts to make this sucky part of the process suck a little less. dammit is free and open source, and has been built around a free and open source ecosystem. As such, programs which the author does not consider free enough have been eschewed as dependencies. This can either mean programs with nonfree licenses or programs which are overly difficult to install and configure -- we believe that access is a part of openness. Details Authors: Camille Scott Contact: camille.scott.w@gmail.com GitHub: https://github.com/camillescott/dammit License: BSD Citation: see static/CITATION","title":"Home"},{"location":"#details","text":"Authors: Camille Scott Contact: camille.scott.w@gmail.com GitHub: https://github.com/camillescott/dammit License: BSD Citation: see static/CITATION","title":"Details"},{"location":"about/","text":"About This page goes a little more in depth on the software and its goals. Motivations Several different factors motivated dammit's development. The first of these was the sea lamprey transcriptome project, which had annotation as a primary goal. Many of dammit's core features were already implemented there, and it seemed a shame not share that work with others in a usable format. Related to this was a lack of workable and easy-to-use existing solutions; in particular, most are meant to be used as protocols and haven't been packaged in an automated format. Licensing was also a big concern -- software used for science should be open source, easily accessible, remixable, and free. Implicit to these motivations is some idea of what a good annotator should look like, in the author's opinion: It should be easy to install and upgrade It should only use Free software It should make use of standard databases It should output in reasonable formats It should be relatively fast It should try to be correct, insofar as any computational approach can be \"correct\" It should give the user some measure of confidence for its results. The Obligatory Flowchart Software Used TransDecoder BUSCO HMMER Infernal LAST crb-blast (for now) pydoit (under the hood) All of these are Free Software, as in freedom and beer Databases Pfam-A Rfam OrthoDB BUSCO databases Uniref90 User-supplied protein databases The last one is important, and sometimes ignored. Conditional Reciprocal Best LAST Building off Richard and co's work on Conditional Reciprocal Best BLAST, I've implemented a new version with Python and LAST -- CRBL. The original lives here . Why?? BLAST is too slooooooow Ruby is yet another dependency to have users install With Python and scikit learn, I have freedom to toy with models (and learn stuff) And, of course, some of these databases are BIG. Doing blastx and tblastn between a reasonably sized transcriptome and Uniref90 is not an experience you want to have. ie, practical concerns. A brief intro to CRBB Reciprocal Best Hits (RBH) is a standard method for ortholog detection Transcriptomes have multiple multiple transcript isoforms, which confounds RBH CRBB uses machine learning to get at this problem CRBB attempts to associate those isoforms with appropriate annotations by learning an appropriate e-value cutoff for different transcript lengths. from http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1004365#s5 CRBL For CRBL, instead of fitting a linear model, we train a model. SVM Naive bayes One limitation is that LAST has no equivalent to tblastn . So, we find the RBHs using the TransDecoder ORFs, and then use the model on the translated transcriptome versus database hits.","title":"About"},{"location":"about/#about","text":"This page goes a little more in depth on the software and its goals.","title":"About"},{"location":"about/#motivations","text":"Several different factors motivated dammit's development. The first of these was the sea lamprey transcriptome project, which had annotation as a primary goal. Many of dammit's core features were already implemented there, and it seemed a shame not share that work with others in a usable format. Related to this was a lack of workable and easy-to-use existing solutions; in particular, most are meant to be used as protocols and haven't been packaged in an automated format. Licensing was also a big concern -- software used for science should be open source, easily accessible, remixable, and free. Implicit to these motivations is some idea of what a good annotator should look like, in the author's opinion: It should be easy to install and upgrade It should only use Free software It should make use of standard databases It should output in reasonable formats It should be relatively fast It should try to be correct, insofar as any computational approach can be \"correct\" It should give the user some measure of confidence for its results.","title":"Motivations"},{"location":"about/#the-obligatory-flowchart","text":"","title":"The Obligatory Flowchart"},{"location":"about/#software-used","text":"TransDecoder BUSCO HMMER Infernal LAST crb-blast (for now) pydoit (under the hood) All of these are Free Software, as in freedom and beer","title":"Software Used"},{"location":"about/#databases","text":"Pfam-A Rfam OrthoDB BUSCO databases Uniref90 User-supplied protein databases The last one is important, and sometimes ignored.","title":"Databases"},{"location":"about/#conditional-reciprocal-best-last","text":"Building off Richard and co's work on Conditional Reciprocal Best BLAST, I've implemented a new version with Python and LAST -- CRBL. The original lives here . Why?? BLAST is too slooooooow Ruby is yet another dependency to have users install With Python and scikit learn, I have freedom to toy with models (and learn stuff) And, of course, some of these databases are BIG. Doing blastx and tblastn between a reasonably sized transcriptome and Uniref90 is not an experience you want to have. ie, practical concerns.","title":"Conditional Reciprocal Best LAST"},{"location":"about/#a-brief-intro-to-crbb","text":"Reciprocal Best Hits (RBH) is a standard method for ortholog detection Transcriptomes have multiple multiple transcript isoforms, which confounds RBH CRBB uses machine learning to get at this problem CRBB attempts to associate those isoforms with appropriate annotations by learning an appropriate e-value cutoff for different transcript lengths. from http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1004365#s5","title":"A brief intro to CRBB"},{"location":"about/#crbl","text":"For CRBL, instead of fitting a linear model, we train a model. SVM Naive bayes One limitation is that LAST has no equivalent to tblastn . So, we find the RBHs using the TransDecoder ORFs, and then use the model on the translated transcriptome versus database hits.","title":"CRBL"},{"location":"database-about/","text":"dammit uses the following databases: Pfam-A Pfam-A is a collection of protein domain profiles for use with profile hidden markov model programs like hmmer . These searches are moderately fast and very sensitive, and the Pfam database is very well curated. Pfam is used during TransDecoder's ORF finding and for annotation assignment. Rfam Rfam is a collection of RNA covariance models for use with programs like Infernal . Covariance models describe RNA secondary structure, and Rfam is a curated database of non-coding RNAs. OrthoDB OrthoDB is a curated database of orthologous genes. It attempts to classify proteins from all major groups of eukaryotes and trace them back to their ancestral ortholog. BUSCO BUSCO databases are collections of \"core\" genes for major domains of life. They are used with an accompanying BUSCO program which assesses the completeness of a genome, transcriptome, or list of genes. There are multiple BUSCO databases, and which one you use depends on your particular organism. Currently available databases are: Metazoa Vertebrata Arthropoda Eukaryota dammit uses the metazoa database by default, but different databases can be used with the --busco-group parameter. You should try to use the database which most closely bounds your organism. uniref90 uniref is a curated collection of most known proteins, clustered at a 90% similarity threshold. This database is comprehensive, and thus quite enormous. dammit does not include it by default due to its size, but it can be installed and used with the --full flag. A command using all of these potential options and databases might look like: dammit databases --install --database-dir /path/to/dbs --full --busco-group arthropoda","title":"About the Databases"},{"location":"database-advanced/","text":"Several of these databases are quite large. Understandably, you probably don't want to download or prepare them again if you already have. There are a few scenarios you might run in to. You already have the databases, and they're all in one place and properly named. Excellent! This is the easiest. You can make use of dammit's --database-dir flag to tell it where to look. When running with --install , it will find the existing files and prep them if necessary.: dammit databases --database-dir <my_database_dir> --install Same as above, but they have different names. dammit expects the databases to be \"properly\" named -- that is, named the same as their original forms. If your databases aren\\'t named the same, you'll need to fix them. But that's okay! We can just soft link them. Let's say you have Pfam-A already, but for some reason its named all-the-models.hmm . You can link them to the proper name like so: cd <my_database_dir> ln -s all-the-models.hmm Pfam-A.hmm If you already formatted it with hmmpress , you can avoid repeating that step as well: ln -s all-the-models.hmm.h3f Pfam-A.hmm.h3f ln -s all-the-models.hmm.h3i Pfam-A.hmm.h3i ln -s all-the-models.hmm.h3m Pfam-A.hmm.h3m ln -s all-the-models.hmm.h3p Pfam-A.hmm.h3p For a complete listing of the expected names, just run the databases command: dammit databases You have the databases, but they're scattered to the virtual winds. The fix here is similar to the above. This time, however, we'll soft link all the databases to one location. If you've run dammit databases , a new directory will have been created at \\$HOME/.dammit/databases . This is where they are stored by default, so we might as well use it! For example: cd $HOME/.dammit/databases ln -s /path/to/all-the-models.hmm Pfam-A.hmm And repeat for all the databases. Now, in the future, you will be able to run dammit without the --database-dir flag. Alternatively, if this all seems like too much of a hassle and you have lots of hard drive space, you can just say \"to hell with it!\" and reinstall everything with: dammit databases --install","title":"Advanced Database Handling"},{"location":"database-usage/","text":"dammit handles databases under the dammit databases subcommand. By default, dammit looks for databases in $HOME/.dammit/databases and will install them there if missing. If you have some of the databases already, you can inform dammit with the --database-dir flag. To check for databases in the default location: dammit databases To check for them in a custom location, you can either use the --database-dir flag: dammit databases --database-dir /path/to/databases or, you can set the DAMMIT_DB_DIR environment variable. The flag will supersede this variable, falling back to the default if neither is set. For example: export DAMMIT_DB_DIR=/path/to/databases This can also be added to your $HOME/.bashrc file to make it persistent. To download and install them into the default directory: dammit databases --install For info on the specific databases used in dammit, see About Databases . For advanced installation and usage instructions, check out the Advanced Database Handling section.","title":"Basic Usage"},{"location":"dev_notes/","text":"For dammit developers dammit! Setting up your local computer for dammit devevelopment We can basically follow the instructions for travis , because we're telling travis to do the same things we want to do on our local computers. Make sure conda is installed. If not, here are instructions: wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b export PATH=\"$HOME/miniconda3/bin:$PATH\" Fork dammit repository to your account. Clone this fork to your local computer, then create a dev branch called testing : git clone https://github.com/username/dammit.git git remote add upstream https://github.com/dib-lab/dammit.git git checkout -b testing git branch Now you are on the testing branch. Keep original repository in the master branch. Make sure it is up-to-date periodically by running: git pull upstream master Set up a Python 3 environment to work in: conda create -n dammit_dev python=3 source activate dammit_dev Install dependencies: conda config --set always_yes yes --set changeps1 no conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda install python numpy pandas numexpr>=2.3.1 khmer>=2.1 sphinx>1.3.1 sphinx_rtd_theme>=0.1.9 pytest pytest-runner doit>=0.29.0 matplotlib shmlast infernal hmmer transdecoder=3.0.1 last busco=3.0.2 parallel bioconductor-seqlogo python setup.py install Last line of the output should be: Finished processing dependencies for dammit==1.0rc2 Lastly, install databases (will install in ~/.dammit/databases/ ) dammit databases --install Output should be: (dammit_dev) campus-019-072:dammit johnsolk$ dammit databases --install Unable to revert mtime: /Library/Fonts # dammit ## a tool for easy de novo transcriptome annotation by Camille Scott **v1.0rc2**, 2018 ## submodule: databases ### Database Install #### Info * Database Directory: /Users/johnsolk/.dammit/databases * Doit Database: /Users/johnsolk/.dammit/databases/databases.doit.db *All database tasks up-to-date.* Nothing to install! Now you are ready to edit and make changes! To-do for dammit [ ] update transdecoder version [ ] orthodb version (other database versions?) [ ] add swissprot [x] change order of conda channels to include conda-forge last [ ] update documentation [ ] add pipeline for accepting .pep file as input (skips transdecoder, transcriptome stats and BUSCO tasks) Versioning A new version is required when a new version of a database is added or a major change happens that changes the commandline interface. Change the VERSION file when this hapens. (Note 11/30/2018: We should make all changes above in the T-do, then move to v1.1) Notes on dammit Written by Camille Scott . See tutorial . Look at pydoit documentation, and Camille's workshop pypi and bioconda (supported method of installation) conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Architecture: Take a look at code and tests in the dammit directory: The core driver of dammit is the dammit/app.py file, which sets up the commandline arguments. Everything happens here. If you want to add an argument, this is where it hapens. There are two subcommand task-handler files: annotate.py and databases.py Tasks are steps being run, separated into different files. For example, the hmmer.py file contains all hmmer tasks. The task handler has on logger, pulls from config to figure out where databases are located (all happens in background), some doit stuff happening Decorators transfer the function's return into a doit function (e.g. line 59 of shell.py) import doit_task then @doit_task databases , 2 pipelines: quick full annotate , more pipelines: uniref1 full nr config.json Can use custom config.json file to include different parameters for the programs run by the tasks, e.g. transdecoder LongOrgs -m 50 , etc. parallel.py hmmer, infernal, lastl, requires gnu parallel (There are instructions for how to runon multi-node hpc, somewhere.) ui.py output for user to markdown formatting for copying/pasting into GitHub issue reporting generate-test-data-.sh re-genreates test data adn puts it in proper dirs TESTS! dammit/tests Run test_databases.py yourself, locally (because databases cannot be cached on travis-ci) makes sure tasks and pipeline run and produce output, they don't all check expected output. some integration output. uses pytest set of tests files testing pydoit tasks is a pain under utils, file to run tasks. give it a list of tasks, it will execute in own directory. functions start with 'test', check assertions fixtures are a means of setting upa consistent environment before running an individual test, e.g. be in a clean directory. tmpdir will create a randomly name temporary directory. make tests for new tasks (Sometimes they will take a long time to run...) test_annotate.py must be run locally by yourself. before pushing release, do both of these make long tests (assumes environment is already setup) travis-ci is building the recipe that lives in the repo make-ci-test , not long and not huge and not requires_datbases Reviewing a PR Tests must pass before merging! Have there been radical changes? (Are you adding things to handler, maybe time to take a step back and make sure code uses reasonable variable names, tests, etc) Does travis build? Try to make commit messages somewhat informative If these all seem reasonable to you, approve! Fix travis: .travis.yml make sure conda env uses right Python fix conda channel order Bioconda https://anaconda.org/bioconda/dammit Recipe: https://github.com/bioconda/bioconda-recipes/blob/master/recipes/dammit/meta.yaml Documentation http://dib-lab.github.io/dammit/ Tutorial from angus 2018","title":"Notes for developers"},{"location":"dev_notes/#for-dammit-developers","text":"dammit!","title":"For dammit developers"},{"location":"dev_notes/#setting-up-your-local-computer-for-dammit-devevelopment","text":"We can basically follow the instructions for travis , because we're telling travis to do the same things we want to do on our local computers. Make sure conda is installed. If not, here are instructions: wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b export PATH=\"$HOME/miniconda3/bin:$PATH\" Fork dammit repository to your account. Clone this fork to your local computer, then create a dev branch called testing : git clone https://github.com/username/dammit.git git remote add upstream https://github.com/dib-lab/dammit.git git checkout -b testing git branch Now you are on the testing branch. Keep original repository in the master branch. Make sure it is up-to-date periodically by running: git pull upstream master Set up a Python 3 environment to work in: conda create -n dammit_dev python=3 source activate dammit_dev Install dependencies: conda config --set always_yes yes --set changeps1 no conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda install python numpy pandas numexpr>=2.3.1 khmer>=2.1 sphinx>1.3.1 sphinx_rtd_theme>=0.1.9 pytest pytest-runner doit>=0.29.0 matplotlib shmlast infernal hmmer transdecoder=3.0.1 last busco=3.0.2 parallel bioconductor-seqlogo python setup.py install Last line of the output should be: Finished processing dependencies for dammit==1.0rc2 Lastly, install databases (will install in ~/.dammit/databases/ ) dammit databases --install Output should be: (dammit_dev) campus-019-072:dammit johnsolk$ dammit databases --install Unable to revert mtime: /Library/Fonts # dammit ## a tool for easy de novo transcriptome annotation by Camille Scott **v1.0rc2**, 2018 ## submodule: databases ### Database Install #### Info * Database Directory: /Users/johnsolk/.dammit/databases * Doit Database: /Users/johnsolk/.dammit/databases/databases.doit.db *All database tasks up-to-date.* Nothing to install! Now you are ready to edit and make changes!","title":"Setting up your local computer for dammit devevelopment"},{"location":"dev_notes/#to-do-for-dammit","text":"[ ] update transdecoder version [ ] orthodb version (other database versions?) [ ] add swissprot [x] change order of conda channels to include conda-forge last [ ] update documentation [ ] add pipeline for accepting .pep file as input (skips transdecoder, transcriptome stats and BUSCO tasks)","title":"To-do for dammit"},{"location":"dev_notes/#versioning","text":"A new version is required when a new version of a database is added or a major change happens that changes the commandline interface. Change the VERSION file when this hapens. (Note 11/30/2018: We should make all changes above in the T-do, then move to v1.1)","title":"Versioning"},{"location":"dev_notes/#notes-on-dammit","text":"Written by Camille Scott . See tutorial . Look at pydoit documentation, and Camille's workshop pypi and bioconda (supported method of installation) conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge","title":"Notes on dammit"},{"location":"dev_notes/#architecture","text":"","title":"Architecture:"},{"location":"dev_notes/#take-a-look-at-code-and-tests-in-the-dammit-directory","text":"The core driver of dammit is the dammit/app.py file, which sets up the commandline arguments. Everything happens here. If you want to add an argument, this is where it hapens. There are two subcommand task-handler files: annotate.py and databases.py Tasks are steps being run, separated into different files. For example, the hmmer.py file contains all hmmer tasks. The task handler has on logger, pulls from config to figure out where databases are located (all happens in background), some doit stuff happening Decorators transfer the function's return into a doit function (e.g. line 59 of shell.py) import doit_task then @doit_task databases , 2 pipelines: quick full annotate , more pipelines: uniref1 full nr","title":"Take a look at code and tests in the dammit directory:"},{"location":"dev_notes/#configjson","text":"Can use custom config.json file to include different parameters for the programs run by the tasks, e.g. transdecoder LongOrgs -m 50 , etc.","title":"config.json"},{"location":"dev_notes/#parallelpy","text":"hmmer, infernal, lastl, requires gnu parallel (There are instructions for how to runon multi-node hpc, somewhere.)","title":"parallel.py"},{"location":"dev_notes/#uipy","text":"output for user to markdown formatting for copying/pasting into GitHub issue reporting generate-test-data-.sh re-genreates test data adn puts it in proper dirs","title":"ui.py"},{"location":"dev_notes/#tests","text":"dammit/tests Run test_databases.py yourself, locally (because databases cannot be cached on travis-ci) makes sure tasks and pipeline run and produce output, they don't all check expected output. some integration output. uses pytest set of tests files testing pydoit tasks is a pain under utils, file to run tasks. give it a list of tasks, it will execute in own directory. functions start with 'test', check assertions fixtures are a means of setting upa consistent environment before running an individual test, e.g. be in a clean directory. tmpdir will create a randomly name temporary directory. make tests for new tasks (Sometimes they will take a long time to run...) test_annotate.py must be run locally by yourself. before pushing release, do both of these make long tests (assumes environment is already setup) travis-ci is building the recipe that lives in the repo make-ci-test , not long and not huge and not requires_datbases","title":"TESTS!"},{"location":"dev_notes/#reviewing-a-pr","text":"Tests must pass before merging! Have there been radical changes? (Are you adding things to handler, maybe time to take a step back and make sure code uses reasonable variable names, tests, etc) Does travis build? Try to make commit messages somewhat informative If these all seem reasonable to you, approve!","title":"Reviewing a PR"},{"location":"dev_notes/#fix-travis","text":".travis.yml make sure conda env uses right Python fix conda channel order","title":"Fix travis:"},{"location":"dev_notes/#bioconda","text":"https://anaconda.org/bioconda/dammit Recipe: https://github.com/bioconda/bioconda-recipes/blob/master/recipes/dammit/meta.yaml","title":"Bioconda"},{"location":"dev_notes/#documentation","text":"http://dib-lab.github.io/dammit/ Tutorial from angus 2018","title":"Documentation"},{"location":"install/","text":"As of version 1.*, the recommended and supported installation platform for dammit is via bioconda , as it greatly simplifies managing dammit's many dependencies. Installing (bio)conda If you already have anaconda installed, proceed to the next step. Otherwise, you can either follow the instructions from bioconda, or if you're on Ubuntu (or most GNU/Linux platforms), install it directly into your home folder with: wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && bash miniconda.sh -b -p $HOME/miniconda echo 'export PATH=\"$HOME/miniconda/bin:$PATH\"' >> $HOME/.bashrc Installing Dammit It's recommended that you use conda environments to separate your packages, though it isn't strictly necessary: conda create -n dammit python=3 source activate dammit Now, add the channels and install dammit: conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda conda install dammit","title":"Bioconda"},{"location":"install/#installing-bioconda","text":"If you already have anaconda installed, proceed to the next step. Otherwise, you can either follow the instructions from bioconda, or if you're on Ubuntu (or most GNU/Linux platforms), install it directly into your home folder with: wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && bash miniconda.sh -b -p $HOME/miniconda echo 'export PATH=\"$HOME/miniconda/bin:$PATH\"' >> $HOME/.bashrc","title":"Installing (bio)conda"},{"location":"install/#installing-dammit","text":"It's recommended that you use conda environments to separate your packages, though it isn't strictly necessary: conda create -n dammit python=3 source activate dammit Now, add the channels and install dammit: conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda conda install dammit","title":"Installing Dammit"},{"location":"system_requirements/","text":"dammit, for now, is officially supported on GNU/Linux systems via bioconda . macOS support will be available via bioconda soon. For the standard pipeline, dammit needs ~18GB of storage space to store its prepared databases, plus a few hundred MB per BUSCO database. For the standard annotation pipeline, we recommend at least 16GB of RAM. This can be reduced by editing LAST parameters via a custom configuration file. The full pipeline, which uses uniref90, needs several hundred GB of space and considerable RAM to prepare the databases. You'll also want either a fat internet connection or a big cup of patience to download uniref. For some species, we have found that the amount of RAM required can be proportional to the size of the transcriptome being annotated.","title":"Requirements"},{"location":"tutorial/","text":"Tutorial Once you have the dependencies installed, it's time to actually annotate something! This guide will take you through a short example on some test data. See this workshop tutorial for further practice with using dammit for annotating a de novo transcriptome assembly. Data First let's download some test data. We'll start small and use a Schizosaccharomyces pombe transcriptome. Make a working directory and move there, and then download the file: mkdir dammit_test cd dammit_test wget ftp://ftp.ebi.ac.uk/pub/databases/pombase/OLD/20170322/FASTA/cdna_nointrons_utrs.fa.gz wget ftp://ftp.ebi.ac.uk/pub/databases/pombase/OLD/20170322/FASTA/pep.fa.gz Decompress the file with gunzip: gunzip cdna_nointrons_utrs.fa.gz pep.fa.gz Databases If you're just starting, you probably haven't downloaded the databases yet. Here we'll install the main databases, as well as the eukaryota BUSCO database for our yeast dataset. This could take a while, so consider walking away and getting yourself a cup of coffee. If you installed dammit into a virtual environment, be sure to activate it first: dammit databases --install --busco-group eukaryota Alternatively, if you happen to have downloaded many of these databases before, you can follow the directions in the databases guide . While the initial download takes a while, once its done, you won't need to do it again. dammit keeps track of the database state and won't repeat work its already completed, even if you accidentally rerun with the --install flag. Annotation Now we'll do a simple run of the annotator. We'll use pep.fa as a user database; this is a toy example, seeing as these proteins came from the same set of transcripts as we're annotating, but they illustrate the usage nicely enough. We'll also specify a non-default BUSCO group. You can replace the argument to --n_threads with however many cores are available on your system in order to speed it up.: dammit annotate cdna_nointrons_utrs.fa --user-databases pep.fa --busco-group eukaryota --n_threads 1 This will take a bit, so go get another cup of coffee...","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"Once you have the dependencies installed, it's time to actually annotate something! This guide will take you through a short example on some test data. See this workshop tutorial for further practice with using dammit for annotating a de novo transcriptome assembly.","title":"Tutorial"},{"location":"tutorial/#data","text":"First let's download some test data. We'll start small and use a Schizosaccharomyces pombe transcriptome. Make a working directory and move there, and then download the file: mkdir dammit_test cd dammit_test wget ftp://ftp.ebi.ac.uk/pub/databases/pombase/OLD/20170322/FASTA/cdna_nointrons_utrs.fa.gz wget ftp://ftp.ebi.ac.uk/pub/databases/pombase/OLD/20170322/FASTA/pep.fa.gz Decompress the file with gunzip: gunzip cdna_nointrons_utrs.fa.gz pep.fa.gz","title":"Data"},{"location":"tutorial/#databases","text":"If you're just starting, you probably haven't downloaded the databases yet. Here we'll install the main databases, as well as the eukaryota BUSCO database for our yeast dataset. This could take a while, so consider walking away and getting yourself a cup of coffee. If you installed dammit into a virtual environment, be sure to activate it first: dammit databases --install --busco-group eukaryota Alternatively, if you happen to have downloaded many of these databases before, you can follow the directions in the databases guide . While the initial download takes a while, once its done, you won't need to do it again. dammit keeps track of the database state and won't repeat work its already completed, even if you accidentally rerun with the --install flag.","title":"Databases"},{"location":"tutorial/#annotation","text":"Now we'll do a simple run of the annotator. We'll use pep.fa as a user database; this is a toy example, seeing as these proteins came from the same set of transcripts as we're annotating, but they illustrate the usage nicely enough. We'll also specify a non-default BUSCO group. You can replace the argument to --n_threads with however many cores are available on your system in order to speed it up.: dammit annotate cdna_nointrons_utrs.fa --user-databases pep.fa --busco-group eukaryota --n_threads 1 This will take a bit, so go get another cup of coffee...","title":"Annotation"}]}